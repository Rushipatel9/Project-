# -*- coding: utf-8 -*-
"""Minor Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yZCyBl9W7aOxacUrd99d3AvYht-ojsiZ
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import warnings
warnings.filterwarnings("ignore")

crime = pd.read_csv("/content/crime_india.csv")
crime

crime.head()

crime.tail()

crime.info()

crime.isna().sum()

crime.columns

# convert the STATE/UT and DISTRICT columns to uppercase
crime['STATE/UT'] = crime['STATE/UT'].str.upper()
crime['DISTRICT'] = crime['DISTRICT'].str.upper()

crime['STATE/UT'].unique()

def state(name):
    state = crime.groupby(['STATE/UT','DISTRICT','YEAR']).sum()['TOTAL IPC CRIMES']
    return state[name]

state('MAHARASHTRA')

crime.info()

"""##Statistical Data Analysis"""

import math
import statistics
import scipy.stats as stats

crime.describe()

crime['TOTAL IPC CRIMES'].mean()

crime.mode()

crime.select_dtypes(include='number').mean() # Select only numeric columns before calculating the mean

numeric_data = crime.select_dtypes(exclude='object')

numeric_data.info()

for col in numeric_data.columns:
    range = crime[col].max() - crime[col].min()
    print('range of %s : %d'%(col,range))

percentiles = crime.describe(percentiles=[.01, .25, .5, .75, .99]).transpose()[["1%", "25%", "50%", "75%", "99%"]]
percentiles

"""##Data visualization"""

plt.figure(figsize=(15,8))
state = crime.groupby('YEAR').sum() # Calculate the sum for each year
plt.pie(state['TOTAL IPC CRIMES'],labels=state.index, # Use the sum of 'TOTAL IPC CRIMES' for the pie chart
        startangle=90,shadow=True,textprops={'fontsize':10,'c':'g'},
        autopct='%0.2f%%')

plt.title('CRIME in INDIA')
plt.show()

crime_totals = crime[['MURDER', 'RAPE', 'KIDNAPPING & ABDUCTION', 'ROBBERY', 'BURGLARY', 'THEFT']].sum()

# Create a pie chart
plt.figure(figsize=(10, 8))
plt.pie(crime_totals.values, labels=crime_totals.index, autopct='%1.1f%%')
plt.title('Proportion of Crimes by Type')
plt.show()

# Group the data by state/union territory and sum the total number of IPC crimes
state_totals = crime.groupby('STATE/UT')['TOTAL IPC CRIMES'].sum()

# Create a bar chart
plt.figure(figsize=(15, 10))
plt.bar(state_totals.index, state_totals.values)
plt.xticks(rotation=90)
plt.xlabel('State/Union Territory')
plt.ylabel('Total IPC Crimes')
plt.title('Total IPC Crimes by State/Union Territory')
plt.show()

# Create a scatter plot of theft and burglary crimes
plt.figure(figsize=(12, 6))
plt.scatter(crime['THEFT'], crime['BURGLARY'])
plt.xlabel('Total Theft Crimes')
plt.ylabel('Total Burglary Crimes')
plt.title('Theft vs. Burglary Crimes')
plt.show()

# Select only the numerical columns for correlation analysis
numerical_crime = crime.select_dtypes(include=['number'])

# Calculate the correlation matrix
numerical_crime.corr()

# Convert 'STATE/UT' and 'DISTRICT' columns to category codes
crime['STATE/UT'] = crime['STATE/UT'].astype('category').cat.codes
crime['DISTRICT'] = crime['DISTRICT'].astype('category').cat.codes

plt.figure(figsize=(25,15))
sns.heatmap(crime.corr(),annot=True,cmap='icefire')

import statsmodels.api as sm
plt.figure(figsize=(15,8))

crime_df = pd.read_csv(r"/content/crime_india.csv", parse_dates=['YEAR'], index_col=['YEAR'])

# Resample data to yearly frequency
crime_yearly = crime_df.resample('Y').sum()

# Plot the time series
plt.plot(crime_yearly.index, crime_yearly['TOTAL IPC CRIMES'])
plt.title('Total IPC Crimes over Time')
plt.xlabel('Year')
plt.ylabel('Number of Cases')
plt.show()

# Decompose the time series into trend, seasonality, and residuals
decomposition = sm.tsa.seasonal_decompose(crime_yearly['TOTAL IPC CRIMES'], model='additive')

# Plot the decomposition results
fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(12, 7))
ax1.plot(crime_yearly.index, decomposition.observed)
ax1.set_ylabel('Observed')
ax2.plot(crime_yearly.index, decomposition.trend)
ax2.set_ylabel('Trend')
ax3.plot(crime_yearly.index, decomposition.seasonal)
ax3.set_ylabel('Seasonal')
ax4.plot(crime_yearly.index, decomposition.resid)
ax4.set_ylabel('Residual')
plt.tight_layout()
plt.show()

# Test for stationarity using the Augmented Dickey-Fuller test
adf_result = sm.tsa.stattools.adfuller(crime_yearly['TOTAL IPC CRIMES'])
print('ADF Statistic:', adf_result[0])
print('p-value:', adf_result[1])
print('Critical Values:', adf_result[4])

# Plot ACF
from statsmodels.graphics.tsaplots import plot_acf
plot_acf(crime_yearly['TOTAL IPC CRIMES'])
plt.show()

# Plot PACF
from statsmodels.graphics.tsaplots import plot_pacf
sm.graphics.tsa.plot_pacf(crime_yearly['TOTAL IPC CRIMES'], lags=5)
plt.title('Partial Autocorrelation Function (PACF)')
plt.xlabel('Lags')
plt.ylabel('Partial Autocorrelation')
plt.show()

# Estimate the autoregressive integrated moving average (ARIMA) model
model = sm.tsa.ARIMA(crime_yearly['TOTAL IPC CRIMES'], order=(1,1,1))
results = model.fit()
print(results.summary())

# Plot the actual values and the predicted values
plt.figure(figsize=(15,8))
plt.plot(crime_yearly.index, crime_yearly['TOTAL IPC CRIMES'], label='Actual')
plt.plot(crime_yearly.index, results.predict(start=0, end=len(crime_yearly)-1), label='Predicted')
plt.title('Actual vs Predicted Total IPC Crimes')
plt.xlabel('Year')
plt.ylabel('Number of Cases')
plt.legend()
plt.show()

# Plot the actual values and the forecasted values
plt.figure(figsize=(15,8))
forecast = results.predict(start=len(crime_yearly), end=len(crime_yearly)+4)
plt.plot(crime_yearly.index, crime_yearly['TOTAL IPC CRIMES'], label='Actual')
plt.plot(forecast.index, forecast, label='Forecast')
plt.title('Actual vs Forecasted Total IPC Crimes')
plt.xlabel('Year')
plt.ylabel('Number of Cases')
plt.legend()
plt.show()

from sklearn.model_selection import train_test_split
# Split the data into train and test sets
train_data, test_data = train_test_split(crime, test_size=0.2, shuffle=False)

# Normalize the data (important for LSTM)
from sklearn.preprocessing import MinMaxScaler
features=numeric_data.columns
scaler = MinMaxScaler()
train_data_scaled = scaler.fit_transform(train_data[features])
test_data_scaled = scaler.transform(test_data[features])

from sklearn.cluster import KMeans

# Apply KMeans for clustering states based on crime rates
kmeans = KMeans(n_clusters=5, random_state=42)
clusters = kmeans.fit_predict(train_data_scaled)

# Add the clusters to the dataset
train_data['Cluster'] = clusters

# Visualize the clustering result
import matplotlib.pyplot as plt
import seaborn as sns

sns.scatterplot(x=train_data['MURDER'], y=train_data['RAPE'], hue=train_data['Cluster'], palette='viridis')
plt.title("Crime Clustering (Murder vs Rape)")
plt.show()

# Ensure train_data_scaled and test_data_scaled are numpy arrays
assert isinstance(train_data_scaled, np.ndarray), "train_data_scaled must be a numpy array"
assert isinstance(test_data_scaled, np.ndarray), "test_data_scaled must be a numpy array"



# Check for variable shadowing

import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# Function to create dataset for LSTM (sliding window approach)
def create_dataset(data, time_step=1):
    # Calculate the total number of samples
    total_samples = data.shape[0] - time_step

    # Create the input features and target labels using NumPy slicing
    X = np.array([data[i:i + time_step] for i in range(total_samples)])  # Input sequences
    Y = np.array([data[i + time_step, 0] for i in range(total_samples)])  # Target labels

    return X, Y



# Load your data (replace 'crime' with the actual dataset you're working with)
numeric_data = crime.select_dtypes(include=[np.number])  # Extract numeric features
features = numeric_data.columns

# Normalize the data
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(numeric_data[features])

# Split the data into train and test sets
train_data_scaled, test_data_scaled = train_test_split(scaled_data, test_size=0.2, shuffle=False)

# Create the dataset using the time step
time_step = 10  # Adjust based on your requirement
# Fix: The built-in function range was overwritten. Delete the variable to use the built-in function.

X_train, y_train = create_dataset(train_data_scaled, time_step)
X_test, y_test = create_dataset(test_data_scaled, time_step)

# Reshape input to be [samples, time steps, features] for LSTM input
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2]))
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2]))

# Build LSTM model
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(units=1))

# Compile model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=2)

def plot_predictions(y_test, predictions, scaler):
    # Inverse transform to return to original values
    # Fix: Reshape y_test to have the same number of features as the original data
    y_test_inverse = scaler.inverse_transform(np.repeat(y_test.reshape(-1, 1), scaler.n_features_in_, axis=1))
    predictions_inverse = scaler.inverse_transform(np.repeat(predictions, scaler.n_features_in_, axis=1))

    # Extract the first column from the inverse transformed arrays
    y_test_inverse = y_test_inverse[:, 0]
    predictions_inverse = predictions_inverse[:, 0]

    plt.plot(y_test_inverse, label='True Data')
    plt.plot(predictions_inverse, label='Predicted Data')
    plt.title('True vs Predicted Data')
    plt.xlabel('Time Step')
    plt.ylabel('Value')
    plt.legend()
    plt.show()
    # Predicting on the test data
predictions = model.predict(X_test)

# Plot predictions vs actual data
plot_predictions(y_test, predictions, scaler)

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

# Function to create dataset for LSTM (sliding window approach)
def create_dataset(data, time_step=1):
    total_samples = data.shape[0] - time_step
    X = np.array([data[i:i + time_step] for i in range(total_samples)])  # Input sequences
    Y = np.array([data[i + time_step, 0] for i in range(total_samples)])  # Target labels
    return X, Y

# Load your data (replace 'crime' with the actual dataset you're working with)
# Assuming you have loaded your dataset into `crime`
numeric_data = crime.select_dtypes(include=[np.number])  # Extract numeric features
features = numeric_data.columns

# Normalize the data
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(numeric_data[features])

# Split the data into train and test sets
train_data_scaled, test_data_scaled = train_test_split(scaled_data, test_size=0.2, shuffle=False)

# Create the dataset using the time step for LSTM
time_step = 10  # Adjust based on your requirement
X_train, y_train = create_dataset(train_data_scaled, time_step)
X_test, y_test = create_dataset(test_data_scaled, time_step)

# Reshape input to be [samples, time steps, features] for LSTM
X_train_lstm = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2]))
X_test_lstm = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2]))

# Build LSTM model
lstm_model = tf.keras.Sequential([
    tf.keras.layers.LSTM(units=64, return_sequences=True, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.LSTM(units=64, return_sequences=False),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(units=1)  # Predicting a single value
])

# Compile LSTM model
lstm_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

# Train the LSTM model
lstm_history = lstm_model.fit(X_train_lstm, y_train, epochs=100, batch_size=32, validation_data=(X_test_lstm, y_test), shuffle=False)

# Evaluate LSTM performance
lstm_val_performance = lstm_model.evaluate(X_test_lstm, y_test, return_dict=True)
print("LSTM Validation Performance:", lstm_val_performance)

# Plot LSTM training history
plt.plot(lstm_history.history['loss'], label='LSTM Train Loss', color='blue')
plt.plot(lstm_history.history['val_loss'], label='LSTM Validation Loss', color='orange')
plt.title('LSTM Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Update the dense model to handle 3D input
inputs = tf.keras.Input(shape=(10, 2))  # Shape matches (time_steps, features)
x = tf.keras.layers.Flatten()(inputs)  # Flatten the input to 2D for Dense layers
x = tf.keras.layers.Dense(units=64, activation='relu')(x)
x = tf.keras.layers.Dense(units=64, activation='relu')(x)
outputs = tf.keras.layers.Dense(units=1)(x)

dense_model = tf.keras.Model(inputs=inputs, outputs=outputs)



# Create single-step windows for Dense model (modify this based on your implementation)
class WindowGenerator:
    def __init__(self, train_data, val_data, test_data, label_column):
        self.train = train_data
        self.val = val_data
        self.test = test_data
        self.label_column = label_column

# Create your window object (replace with your actual data windowing implementation)
single_step_window = WindowGenerator(train_data_scaled, test_data_scaled, test_data_scaled, label_column=0)

class WindowGenerator:
    def __init__(self, train_data, val_data, test_data, label_column):
        self.label_column = label_column
        self.train = self.create_dataset(train_data)
        self.val = self.create_dataset(val_data)
        self.test = self.create_dataset(test_data)

    def create_dataset(self, data):
        # Example: Create sequences for LSTM training
        time_step = 10  # Define your time step here
        X, Y = [], []
        for i in range(len(data) - time_step):
            X.append(data[i:i + time_step])
            Y.append(data[i + time_step, self.label_column])  # Assuming the label is in the specified column
        return np.array(X), np.array(Y)

# Create your window object
single_step_window = WindowGenerator(train_data_scaled, test_data_scaled, test_data_scaled, label_column=0)

# Check the shape of your data
print("Training data shape:", single_step_window.train[0].shape)  # Should print (time_steps, features)
print("Training labels shape:", single_step_window.train[1].shape)  # Should print (samples,)

print(type(single_step_window.train))
print(single_step_window.train)  # Inspect the contents
inputs = tf.keras.Input(shape=(single_step_window.train[0].shape[1],))  # Use the first element
# Accessing the correct shape from the tuple
train_data_shape = single_step_window.train[0].shape  # Assuming this is your data
inputs = tf.keras.Input(shape=(train_data_shape[1],))  # Set the input shape based on your training data

# Define the dense model using the Functional API (outside the training loop)
train_data_shape = single_step_window.train[0].shape  # Access the first element of the tuple
inputs = tf.keras.Input(shape=(train_data_shape[1], train_data_shape[2]))  # Set the input shape to (10, 33)
x = tf.keras.layers.Dense(units=64, activation='relu')(inputs)
x = tf.keras.layers.Dense(units=64, activation='relu')(x)
outputs = tf.keras.layers.Dense(units=1)(x)

dense_model = tf.keras.Model(inputs=inputs, outputs=outputs)

# Compile the model
dense_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

# Check for NaN values
print("None in train_data_scaled:", np.any(np.isnan(train_data_scaled)))
print("None in test_data_scaled:", np.any(np.isnan(test_data_scaled)))

# Train the model
num_epochs = 50
batch_size = 32

# Create history list to store the training history for plotting later
history_list = []

# Training loop
for epoch in range(num_epochs):
    print(f"Epoch {epoch + 1}/{num_epochs}")

    # Train the model and store history
    history = dense_model.fit(single_step_window.train[0],  # Use the first element for features
                               single_step_window.train[1],  # Use the second element for labels
                               epochs=1,
                               batch_size=batch_size,
                               validation_data=(single_step_window.val[0], single_step_window.val[1]),  # Use the first and second elements for validation
                               verbose=1)

    # Append history for this epoch
    history_list.append(history.history)

    # Optionally, print metrics after each epoch
    train_loss, train_mae = dense_model.evaluate(single_step_window.train[0], single_step_window.train[1], verbose=0)
    val_loss, val_mae = dense_model.evaluate(single_step_window.val[0], single_step_window.val[1], verbose=0)

    print(f"Training Loss: {train_loss:.4f}, Training MAE: {train_mae:.4f}")
    print(f"Validation Loss: {val_loss:.4f}, Validation MAE: {val_mae:.4f}")

# Optionally, you can plot the training history
# Combine loss values from each epoch
train_losses = [h['loss'][0] for h in history_list]
val_losses = [h['val_loss'][0] for h in history_list]

plt.plot(train_losses, label='Train Loss', color='blue')
plt.plot(val_losses, label='Validation Loss', color='orange')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

def create_lstm_model(input_shape):
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.LSTM(50, return_sequences=True, input_shape=input_shape))
    model.add(tf.keras.layers.LSTM(50))
    model.add(tf.keras.layers.Dense(1))
    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])
    return model
# Define the dense model using the Functional API (outside the training loop)
inputs = tf.keras.Input(shape=(single_step_window.train[0].shape[1], single_step_window.train[0].shape[2]))
x = tf.keras.layers.Dense(units=64, activation='relu')(inputs)
x = tf.keras.layers.Dense(units=64, activation='relu')(x)
outputs = tf.keras.layers.Dense(units=1)(x)

dense_model = tf.keras.Model(inputs=inputs, outputs=outputs)

# Compile the model
dense_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

# Check for NaN values
print("None in train_data_scaled:", np.any(np.isnan(train_data_scaled)))
print("None in test_data_scaled:", np.any(np.isnan(test_data_scaled)))

# Train the Dense model and collect history
dense_history = dense_model.fit(
    single_step_window.train[0], # Use the first element of the tuple for the training data
    single_step_window.train[1], # Use the second element of the tuple for the training labels
    epochs=100,
    batch_size=32,
    validation_data=(single_step_window.val[0], single_step_window.val[1]), # Use the first and second elements for validation data and labels
    verbose=0
)

# Train LSTM model
lstm_model = create_lstm_model((X_train_lstm.shape[1], X_train_lstm.shape[2]))
lstm_history = lstm_model.fit(
    X_train_lstm,
    y_train,
    epochs=100,
    batch_size=32,
    validation_data=(X_test_lstm, y_test),
    verbose=0
)

# Collect metrics
dense_mae = dense_history.history['mae']  # MAE from Dense model
lstm_mae = lstm_history.history['mae']     # MAE from LSTM model

# Plotting MAE comparison
plt.figure(figsize=(10, 6))
plt.plot(dense_mae, label='Dense Model MAE', color='blue')
plt.plot(lstm_mae, label='LSTM Model MAE', color='orange')
plt.title('Model MAE Comparison')
plt.xlabel('Epochs')
plt.ylabel('Mean Absolute Error')
plt.legend()
plt.grid()
plt.show()

# Compile the dense model with both 'mae' and 'mse' as metrics
dense_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae', 'mse'])

# Create and compile the LSTM model
lstm_model = create_lstm_model((X_train_lstm.shape[1], X_train_lstm.shape[2]))
lstm_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae', 'mse'])

# Train the dense model and collect history
dense_history = dense_model.fit(
    single_step_window.train[0],  # Training data
    single_step_window.train[1],  # Training labels
    epochs=50,
    batch_size=32,
    validation_data=(single_step_window.val[0], single_step_window.val[1]),  # Validation data and labels
    verbose=0
)

# Train the LSTM model and collect history
lstm_history = lstm_model.fit(
    X_train_lstm,
    y_train,
    epochs=50,
    batch_size=32,
    validation_data=(X_test_lstm, y_test),
    verbose=0
)

# Check the available keys in the LSTM history
print("Available keys in LSTM history:", lstm_history.history.keys())

# Collect metrics from the histories
dense_mae = dense_history.history['mae']
dense_mse = dense_history.history['mse']
dense_loss = dense_history.history['loss']

# Check if 'mse' exists before accessing it
if 'mse' in lstm_history.history:
    lstm_mae = lstm_history.history['mae']
    lstm_mse = lstm_history.history['mse']
    lstm_loss = lstm_history.history['loss']
else:
    print("'mse' not found in LSTM history. Available metrics:", lstm_history.history.keys())

plt.figure(figsize=(10, 6))
plt.plot(dense_mae, label='Dense Model MAE', color='blue')
plt.plot(lstm_mae, label='LSTM Model MAE', color='orange')
plt.title('MAE Comparison')
plt.xlabel('Epochs')
plt.ylabel('Mean Absolute Error')
plt.legend()
plt.grid()
plt.show()

plt.figure(figsize=(10, 6))
plt.plot(dense_mse, label='Dense Model MSE', color='blue')
plt.plot(lstm_mse, label='LSTM Model MSE', color='orange')
plt.title('MSE Comparison')
plt.xlabel('Epochs')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.grid()
plt.show()

dense_rmse = [np.sqrt(mse) for mse in dense_mse]
lstm_rmse = [np.sqrt(mse) for mse in lstm_mse]

# Plot RMSE
plt.figure(figsize=(10, 6))
plt.plot(dense_rmse, label='Dense Model RMSE', color='blue')
plt.plot(lstm_rmse, label='LSTM Model RMSE', color='orange')
plt.title('RMSE Comparison')
plt.xlabel('Epochs')
plt.ylabel('Root Mean Squared Error')
plt.legend()
plt.grid()
plt.show()

import matplotlib.pyplot as plt

# Set up a figure for the plots
plt.figure(figsize=(12, 5))

# Plot for Dense Model
plt.subplot(1, 2, 1)
plt.plot(dense_loss, label='Dense Train Loss')
plt.plot(dense_history.history['val_loss'], label='Dense Val Loss', linestyle='--')
plt.plot(dense_mae, label='Dense Train MAE')
plt.plot(dense_history.history['val_mae'], label='Dense Val MAE', linestyle='--')
plt.title('Dense Model Training History')
plt.xlabel('Epochs')
plt.ylabel('Loss / MAE')
plt.legend()
plt.grid()

# Plot for LSTM Model
plt.subplot(1, 2, 2)
plt.plot(lstm_history.history['loss'], label='LSTM Train Loss')
plt.plot(lstm_history.history['val_loss'], label='LSTM Val Loss', linestyle='--')
plt.plot(lstm_mae, label='LSTM Train MAE')
plt.plot(lstm_history.history['val_mae'], label='LSTM Val MAE', linestyle='--')
plt.title('LSTM Model Training History')
plt.xlabel('Epochs')
plt.ylabel('Loss / MAE')
plt.legend()
plt.grid()

plt.tight_layout()
plt.show()

# Evaluate Dense Model
dense_test_metrics = dense_model.evaluate(single_step_window.test[0], single_step_window.test[1], verbose=0)
dense_test_loss, dense_test_mae, dense_test_mse = dense_test_metrics

# Evaluate LSTM Model
lstm_test_metrics = lstm_model.evaluate(X_test_lstm, y_test, verbose=0)
lstm_test_loss, lstm_test_mae, lstm_test_mse = lstm_test_metrics

# Print final metrics
print("Dense Model Test Metrics:")
print(f"  Loss: {dense_test_loss:.4f}, MAE: {dense_test_mae:.4f}, MSE: {dense_test_mse:.4f}")

print("LSTM Model Test Metrics:")
print(f"  Loss: {lstm_test_loss:.4f}, MAE: {lstm_test_mae:.4f}, MSE: {lstm_test_mse:.4f}")

# Assuming y_train was your target variable during scaler fitting
scaler.fit(y_train.reshape(-1, 1))  # Fit on the target variable if needed

# Now, if you want to inverse transform a single prediction, you can reshape it to 2D
dense_predictions_rescaled = scaler.inverse_transform(dense_predictions_reshaped.reshape(-1, 1))  # 2D shape for one feature
lstm_predictions_rescaled = scaler.inverse_transform(lstm_predictions_reshaped.reshape(-1, 1))  # 2D shape for one feature

print("Shape of dense predictions:", dense_predictions.shape)
print("Shape of dense predictions reshaped:", dense_predictions_reshaped.shape)
print("Shape of LSTM predictions:", lstm_predictions.shape)
print("Shape of LSTM predictions reshaped:", lstm_predictions_reshaped.shape)

# Use this check to ensure you're reshaping correctly

# Fit the scaler on the target variable if not already done
scaler.fit(y_train.reshape(-1, 1))  # Ensure y_train is reshaped correctly

# Reshape and average dense predictions
dense_predictions_flattened = np.mean(dense_predictions, axis=1)  # Shape will be (1794,)

# Rescale predictions
dense_predictions_rescaled = scaler.inverse_transform(dense_predictions_flattened.reshape(-1, 1))  # Reshape for scaler
lstm_predictions_rescaled = scaler.inverse_transform(lstm_predictions.reshape(-1, 1))  # LSTM already in correct shape

# Print the shapes for verification
print("Shape of dense predictions rescaled:", dense_predictions_rescaled.shape)
print("Shape of LSTM predictions rescaled:", lstm_predictions_rescaled.shape)

print("Shape of dense predictions:", dense_predictions.shape)
print("Shape of dense predictions reshaped:", dense_predictions_reshaped.shape)
print("Shape of LSTM predictions:", lstm_predictions.shape)
print("Shape of LSTM predictions reshaped:", lstm_predictions_reshaped.shape)

# Fit the scaler on the target variable if not already done
scaler.fit(y_train.reshape(-1, 1))

# Average dense predictions over time steps
dense_predictions_avg = np.mean(dense_predictions, axis=1)  # Shape will be (1794, 1)

# Rescale predictions
dense_predictions_rescaled = scaler.inverse_transform(dense_predictions_avg.reshape(-1, 1))  # Reshape for scaler
lstm_predictions_rescaled = scaler.inverse_transform(lstm_predictions)  # Already (1794, 1)

# Print the shapes for verification
print("Shape of dense predictions rescaled:", dense_predictions_rescaled.shape)  # Should be (1794, 1)
print("Shape of LSTM predictions rescaled:", lstm_predictions_rescaled.shape)  # Should be (1794, 1)

# Rescale the test target variable (if not already done)
y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()

# Calculate metrics for Dense model
dense_mae_test = mean_absolute_error(y_test_rescaled, dense_predictions_rescaled)
dense_mse_test = mean_squared_error(y_test_rescaled, dense_predictions_rescaled)
dense_rmse_test = np.sqrt(dense_mse_test)

# Calculate metrics for LSTM model
lstm_mae_test = mean_absolute_error(y_test_rescaled, lstm_predictions_rescaled)
lstm_mse_test = mean_squared_error(y_test_rescaled, lstm_predictions_rescaled)
lstm_rmse_test = np.sqrt(lstm_mse_test)

# Print the results
print(f"Dense Model - Test MAE: {dense_mae_test:.4f}, Test RMSE: {dense_rmse_test:.4f}")
print(f"LSTM Model - Test MAE: {lstm_mae_test:.4f}, Test RMSE: {lstm_rmse_test:.4f}")

import matplotlib.pyplot as plt

# Plotting Dense Model predictions
plt.figure(figsize=(14, 6))
plt.subplot(1, 2, 1)
plt.plot(y_test_rescaled, label='True Values', color='blue')
plt.plot(dense_predictions_rescaled, label='Dense Model Predictions', color='orange')
plt.title('Dense Model Predictions vs True Values')
plt.xlabel('Samples')
plt.ylabel('Values')
plt.legend()

# Plotting LSTM Model predictions
plt.subplot(1, 2, 2)
plt.plot(y_test_rescaled, label='True Values', color='blue')
plt.plot(lstm_predictions_rescaled, label='LSTM Model Predictions', color='green')
plt.title('LSTM Model Predictions vs True Values')
plt.xlabel('Samples')
plt.ylabel('Values')
plt.legend()

plt.tight_layout()
plt.show()

# Define a threshold for classification if necessary
threshold = 0.5  # Adjust based on your specific context

# Convert predictions into binary classes based on the threshold
dense_predictions_classes = (dense_predictions_rescaled > threshold).astype(int)
lstm_predictions_classes = (lstm_predictions_rescaled > threshold).astype(int)

# Assuming y_test is binary as well
y_test_classes = (y_test_rescaled > threshold).astype(int)

from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score

# Confusion Matrix for Dense Model
dense_confusion = confusion_matrix(y_test_classes, dense_predictions_classes)
dense_f1 = f1_score(y_test_classes, dense_predictions_classes)
dense_precision = precision_score(y_test_classes, dense_predictions_classes)
dense_recall = recall_score(y_test_classes, dense_predictions_classes)

# Confusion Matrix for LSTM Model
lstm_confusion = confusion_matrix(y_test_classes, lstm_predictions_classes)
lstm_f1 = f1_score(y_test_classes, lstm_predictions_classes)
lstm_precision = precision_score(y_test_classes, lstm_predictions_classes)
lstm_recall = recall_score(y_test_classes, lstm_predictions_classes)

# Print the results
print("Dense Model Confusion Matrix:\n", dense_confusion)
print(f"Dense Model F1 Score: {dense_f1:.4f}, Precision: {dense_precision:.4f}, Recall: {dense_recall:.4f}")

print("LSTM Model Confusion Matrix:\n", lstm_confusion)
print(f"LSTM Model F1 Score: {lstm_f1:.4f}, Precision: {lstm_precision:.4f}, Recall: {lstm_recall:.4f}")

# Regression Metrics for Dense Model
dense_mae_test = mean_absolute_error(y_test_rescaled, dense_predictions_rescaled)
dense_mse_test = mean_squared_error(y_test_rescaled, dense_predictions_rescaled)
dense_rmse_test = np.sqrt(dense_mse_test)

# Regression Metrics for LSTM Model
lstm_mae_test = mean_absolute_error(y_test_rescaled, lstm_predictions_rescaled)
lstm_mse_test = mean_squared_error(y_test_rescaled, lstm_predictions_rescaled)
lstm_rmse_test = np.sqrt(lstm_mse_test)

# Print the regression metrics
print(f"Dense Model - Test MAE: {dense_mae_test:.4f}, Test RMSE: {dense_rmse_test:.4f}")
print(f"LSTM Model - Test MAE: {lstm_mae_test:.4f}, Test RMSE: {lstm_rmse_test:.4f}")

import pandas as pd

# Prepare data for the table
data = {
    "Model": ["Dense Model", "LSTM Model"],
    "MAE": [dense_mae_test, lstm_mae_test],
    "RMSE": [dense_rmse_test, lstm_rmse_test],
    "F1 Score": [dense_f1, lstm_f1],
    "Precision": [dense_precision, lstm_precision],
    "Recall": [dense_recall, lstm_recall],
    "Confusion Matrix": [dense_confusion.tolist(), lstm_confusion.tolist()]
}

# Create a DataFrame
summary_df = pd.DataFrame(data)

# Print the summary DataFrame
print(summary_df)

# Example: Forecast the next 'n' steps
n_steps = 10  # Number of future steps you want to forecast

# Prepare the input data for forecasting (last known data point)
last_known_dense_input = single_step_window.train[0][-1:]  # Last input from training
last_known_lstm_input = X_train_lstm[-1:]  # Last input for LSTM

# Predict using Dense model
dense_forecast = dense_model.predict(last_known_dense_input)

# Predict using LSTM model
lstm_forecast = lstm_model.predict(last_known_lstm_input)

# Reshape if necessary (similar to previous steps)
dense_forecast_rescaled = scaler.inverse_transform(dense_forecast.reshape(-1, 1))
lstm_forecast_rescaled = scaler.inverse_transform(lstm_forecast.reshape(-1, 1))

print("Dense Model Forecast:", dense_forecast_rescaled)
print("LSTM Model Forecast:", lstm_forecast_rescaled)

from sklearn.cluster import KMeans
import numpy as np

# Example data: use predictions or original data for clustering
data_for_clustering = np.concatenate([dense_predictions_reshaped, lstm_predictions_reshaped], axis=0)

# Reshape if necessary (2D array)
data_for_clustering = data_for_clustering.reshape(-1, 1)  # Adjust shape if needed

# Define the number of clusters
n_clusters = 3  # Adjust based on your needs

# Create KMeans model
kmeans = KMeans(n_clusters=n_clusters, random_state=42)

# Fit the model
kmeans.fit(data_for_clustering)

# Get cluster labels
cluster_labels = kmeans.labels_

# Optionally, you can get the cluster centers
cluster_centers = kmeans.cluster_centers_

# Print the cluster labels and centers
print("Cluster Labels:", cluster_labels)
print("Cluster Centers:", cluster_centers)

import matplotlib.pyplot as plt

# Plot the clustered data
plt.scatter(data_for_clustering, np.zeros_like(data_for_clustering), c=cluster_labels, cmap='viridis', marker='o')
plt.scatter(cluster_centers, np.zeros_like(cluster_centers), c='red', s=200, alpha=0.5, marker='X', label='Centroids')
plt.title('K-Means Clustering Results')
plt.xlabel('Data Points')
plt.yticks([])  # Hide y-axis
plt.legend()
plt.show()

import matplotlib.pyplot as plt

# Assume y_test is your actual test values
plt.figure(figsize=(14, 7))

# Plot actual values
plt.plot(y_test_rescaled, label='Actual Values', color='blue')

# Plot LSTM predictions
plt.plot(lstm_predictions_rescaled, label='LSTM Predictions', color='orange')

plt.title('Actual vs. LSTM Predictions')
plt.xlabel('Time Steps')
plt.ylabel('Value')
plt.legend()
plt.grid()
plt.show()

# Calculate residuals
residuals = y_test_rescaled - lstm_predictions_rescaled

# Convert residuals to a 1D array
residuals = residuals.flatten()

plt.figure(figsize=(14, 7))
plt.scatter(range(len(residuals)), residuals, color='red')
plt.axhline(0, color='black', linestyle='--', linewidth=1)
plt.title('Residuals of LSTM Predictions')
plt.xlabel('Time Steps')
plt.ylabel('Residuals')
plt.grid()
plt.show()

# Assuming data_for_clustering is your clustered data
plt.figure(figsize=(14, 7))
plt.scatter(data_for_clustering, np.zeros_like(data_for_clustering), c=cluster_labels, cmap='viridis', marker='o', alpha=0.5)

# Plot centroids
plt.scatter(cluster_centers, np.zeros_like(cluster_centers), c='red', s=200, alpha=0.5, marker='X', label='Centroids')

plt.title('K-Means Clustering of Predictions')
plt.xlabel('Data Points')
plt.yticks([])  # Hide y-axis
plt.legend()
plt.grid()
plt.show()

# Assuming data_for_clustering has multiple features (for example, predictions from both models)
from sklearn.decomposition import PCA

# Reduce to 1 dimension because data_for_clustering has only 1 feature
pca = PCA(n_components=1)
data_2d = pca.fit_transform(data_for_clustering)

plt.figure(figsize=(14, 7))
plt.scatter(data_2d[:, 0], data_2d[:, 0], c=cluster_labels, cmap='viridis', marker='o', alpha=0.5)

# Plot centroids in reduced space
centroids_2d = pca.transform(cluster_centers)
plt.scatter(centroids_2d[:, 0], centroids_2d[:, 0], c='red', s=200, alpha=0.5, marker='X', label='Centroids')

plt.title('K-Means Clustering in 2D Space')
plt.xlabel('PCA Feature 1')
plt.ylabel('PCA Feature 2')
plt.legend()
plt.grid()
plt.show()

plt.figure(figsize=(14, 7))
plt.scatter(range(len(y_test_rescaled)), y_test_rescaled, label='Actual Values', color='blue', alpha=0.5)
plt.scatter(range(len(lstm_predictions_rescaled)), lstm_predictions_rescaled, label='LSTM Predictions', color='orange', alpha=0.5)


plt.title('Actual vs. LSTM Predictions with Clusters')
plt.xlabel('Time Steps')
plt.ylabel('Value')
plt.legend()
plt.grid()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

# Assuming you have trained your Dense and LSTM models, and scaler is already fitted to your data

# Example: Forecast the next 'n' steps
n_steps = 10  # Number of future steps you want to forecast

# Prepare the input data for forecasting (last known data point)
last_known_dense_input = single_step_window.train[0][-1:]  # Last input from training for Dense model
last_known_lstm_input = X_train_lstm[-1:]  # Last input for LSTM model

# Forecast using Dense model
dense_forecast = dense_model.predict(last_known_dense_input)

# Forecast using LSTM model
lstm_forecast = lstm_model.predict(last_known_lstm_input)

# Reshape if necessary (similar to previous steps)
dense_forecast_rescaled = scaler.inverse_transform(dense_forecast.reshape(-1, 1))
lstm_forecast_rescaled = scaler.inverse_transform(lstm_forecast.reshape(-1, 1))

# Print the forecasts
print("Dense Model Forecast:", dense_forecast_rescaled.flatten())
print("LSTM Model Forecast:", lstm_forecast_rescaled.flatten())

# Visualize the forecasts
# Create an array for future time steps
future_steps = np.arange(len(y_test), len(y_test) + n_steps)

# Combine actual test data with forecasts for visualization
combined_y = np.concatenate((y_test, dense_forecast_rescaled.flatten(), lstm_forecast_rescaled.flatten()))

# Create a DataFrame for better visualization
# Use np.full to create arrays of NaNs with the correct lengths
forecast_df = pd.DataFrame({
    'Time Step': np.concatenate((np.arange(len(y_test)), future_steps)),
    'Actual': np.concatenate((y_test, np.full(n_steps, np.nan))),  # Actual data with NaN for forecasted steps
    'Dense Forecast': np.concatenate((np.full(len(y_test), np.nan), dense_forecast_rescaled.flatten())),
    'LSTM Forecast': np.concatenate((np.full(len(y_test), np.nan), np.full(n_steps,lstm_forecast_rescaled.flatten()[0]))) # Repeat the LSTM forecast value to match length
})

# Plotting the results
plt.figure(figsize=(14, 7))
plt.plot(forecast_df['Time Step'], forecast_df['Actual'], label='Actual', color='blue', marker='o')
plt.plot(forecast_df['Time Step'], forecast_df['Dense Forecast'], label='Dense Forecast', color='orange', marker='x')
plt.plot(forecast_df['Time Step'], forecast_df['LSTM Forecast'], label='LSTM Forecast', color='green', marker='s')
plt.title('Forecast Comparison: Dense vs LSTM Models')
plt.xlabel('Time Step')
plt.ylabel('Value')
plt.axvline(x=len(y_test)-1, color='red', linestyle='--', label='Forecast Start')
plt.legend()
plt.show()

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt

# Load your dataset (replace 'crime_data.csv' with the actual file path)
data = pd.read_csv('/content/crime_india.csv')

# Display the first few rows of the dataset to understand its structure
print(data.head())

# Select the relevant numeric columns (crime data) for clustering
crime_columns = [
    'MURDER', 'ATTEMPT TO MURDER', 'CULPABLE HOMICIDE NOT AMOUNTING TO MURDER',
    'RAPE', 'CUSTODIAL RAPE', 'OTHER RAPE', 'KIDNAPPING & ABDUCTION',
    'KIDNAPPING AND ABDUCTION OF WOMEN AND GIRLS', 'KIDNAPPING AND ABDUCTION OF OTHERS',
    'DACOITY', 'PREPARATION AND ASSEMBLY FOR DACOITY', 'ROBBERY',
    'BURGLARY', 'THEFT', 'AUTO THEFT', 'OTHER THEFT', 'RIOTS',
    'CRIMINAL BREACH OF TRUST', 'CHEATING', 'COUNTERFIETING',
    'ARSON', 'HURT/GREVIOUS HURT', 'DOWRY DEATHS', 'ASSAULT ON WOMEN WITH INTENT TO OUTRAGE HER MODESTY',
    'INSULT TO MODESTY OF WOMEN', 'CRUELTY BY HUSBAND OR HIS RELATIVES',
    'IMPORTATION OF GIRLS FROM FOREIGN COUNTRIES', 'CAUSING DEATH BY NEGLIGENCE',
    'OTHER IPC CRIMES', 'TOTAL IPC CRIMES'
]

# Extract the crime data from the dataset
X = data[crime_columns]

# Normalize the data using MinMaxScaler
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Apply KMeans clustering
kmeans = KMeans(n_clusters=5, random_state=42)  # Change n_clusters as needed
data['Cluster'] = kmeans.fit_predict(X_scaled)

# Display the dataset with the new 'Cluster' column
print(data[['STATE/UT', 'DISTRICT', 'Cluster']].head())

# Plot the clusters (if you want to visualize using 2 features for simplicity)
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=data['Cluster'], cmap='viridis')
plt.title('KMeans Clustering of Crime Data')
plt.xlabel('MURDER (normalized)')
plt.ylabel('ATTEMPT TO MURDER (normalized)')
plt.show()

# Save the data with cluster information
data.to_csv('crime_data_with_clusters.csv', index=False)

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import seaborn as sns

# Load your dataset (replace 'crime_data.csv' with the actual file path)
data = pd.read_csv('/content/crime_india.csv')

# Select the relevant numeric columns (crime data) for clustering
crime_columns = [
    'MURDER', 'ATTEMPT TO MURDER', 'CULPABLE HOMICIDE NOT AMOUNTING TO MURDER',
    'RAPE', 'CUSTODIAL RAPE', 'OTHER RAPE', 'KIDNAPPING & ABDUCTION',
    'KIDNAPPING AND ABDUCTION OF WOMEN AND GIRLS', 'KIDNAPPING AND ABDUCTION OF OTHERS',
    'DACOITY', 'PREPARATION AND ASSEMBLY FOR DACOITY', 'ROBBERY',
    'BURGLARY', 'THEFT', 'AUTO THEFT', 'OTHER THEFT', 'RIOTS',
    'CRIMINAL BREACH OF TRUST', 'CHEATING', 'COUNTERFIETING',
    'ARSON', 'HURT/GREVIOUS HURT', 'DOWRY DEATHS', 'ASSAULT ON WOMEN WITH INTENT TO OUTRAGE HER MODESTY',
    'INSULT TO MODESTY OF WOMEN', 'CRUELTY BY HUSBAND OR HIS RELATIVES',
    'IMPORTATION OF GIRLS FROM FOREIGN COUNTRIES', 'CAUSING DEATH BY NEGLIGENCE',
    'OTHER IPC CRIMES', 'TOTAL IPC CRIMES'
]

# Function to normalize the crime data and apply KMeans clustering for each state
def kmeans_per_state(data, n_clusters=3):
    states = data['STATE/UT'].unique()  # Get unique states
    results = []  # Store results for each state

    for state in states:
        state_data = data[data['STATE/UT'] == state]  # Filter data by state
        X = state_data[crime_columns]  # Select crime data for the state

        # Normalize the data
        scaler = MinMaxScaler()
        X_scaled = scaler.fit_transform(X)

        # Apply KMeans clustering
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        clusters = kmeans.fit_predict(X_scaled)

        # Add the cluster information back to the data
        state_data['Cluster'] = clusters
        results.append(state_data)  # Append results for this state

    # Concatenate all the state dataframes with cluster info
    return pd.concat(results, axis=0)

# Apply the function to get the clustered data
clustered_data = kmeans_per_state(data)

# Function to plot the crime data for each state
def plot_state_clusters(clustered_data, state_name):
    state_data = clustered_data[clustered_data['STATE/UT'] == state_name]  # Filter data for the state
    plt.figure(figsize=(10, 6))

    # Plotting the districts with clusters and total IPC crimes
    sns.barplot(x='DISTRICT', y='TOTAL IPC CRIMES', hue='Cluster', data=state_data)

    plt.title(f'Total IPC Crimes by District in {state_name}')
    plt.xlabel('District')
    plt.ylabel('Total IPC Crimes')
    plt.xticks(rotation=90)
    plt.legend(title='Cluster')
    plt.show()

# Iterate over all states and plot clusters for each state
states = clustered_data['STATE/UT'].unique()
for state in states:
    plot_state_clusters(clustered_data, state)

# Save the clustered data to a CSV file
clustered_data.to_csv('crime_data_with_clusters.csv', index=False)

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load your dataset (replace 'crime_data.csv' with the actual file path)
data = pd.read_csv('/content/crime_india.csv')

# Select relevant columns (district, state, and total IPC crimes)
data = data[['STATE/UT', 'DISTRICT', 'TOTAL IPC CRIMES']]

# Function to calculate crime percentage for each district in a state
def calculate_crime_percentage(data):
    # Group data by state
    states = data['STATE/UT'].unique()  # Get unique states
    results = []  # To store each state's data with percentages

    for state in states:
        # Filter the data by state
        state_data = data[data['STATE/UT'] == state]

        # Calculate total crimes for the state
        total_crimes_in_state = state_data['TOTAL IPC CRIMES'].sum()

        # Calculate crime percentage for each district
        state_data['Crime Percentage'] = (state_data['TOTAL IPC CRIMES'] / total_crimes_in_state) * 100

        # Append the result for this state
        results.append(state_data)

    # Concatenate all state dataframes
    return pd.concat(results, axis=0)

# Apply the function to calculate crime percentage for each district
crime_percentage_data = calculate_crime_percentage(data)

# Function to plot crime percentages for a given state
def plot_state_crime_percentage(crime_percentage_data, state_name):
    state_data = crime_percentage_data[crime_percentage_data['STATE/UT'] == state_name]  # Filter data by state

    plt.figure(figsize=(10, 6))
    # Barplot of Crime Percentage for each district in the state
    sns.barplot(x='DISTRICT', y='Crime Percentage', data=state_data, palette='coolwarm')

    plt.title(f'Crime Percentage by District in {state_name}')
    plt.xlabel('District')
    plt.ylabel('Crime Percentage (%)')
    plt.xticks(rotation=90)
    plt.tight_layout()
    plt.show()

# Iterate over all states and plot crime percentages for each state
states = crime_percentage_data['STATE/UT'].unique()
for state in states:
    plot_state_crime_percentage(crime_percentage_data, state)

# Save the result to a CSV file for reference
crime_percentage_data.to_csv('crime_percentage_data.csv', index=False)

import pandas as pd
import matplotlib.pyplot as plt

# Load your dataset (replace 'crime_data.csv' with the actual file path)
data = pd.read_csv('/content/crime_india.csv')

# Select relevant columns (district, state, and total IPC crimes)
data = data[['STATE/UT', 'DISTRICT', 'TOTAL IPC CRIMES']]

# Function to calculate crime percentage for each district in a state
def calculate_crime_percentage(data):
    # Group data by state
    states = data['STATE/UT'].unique()  # Get unique states
    results = []  # To store each state's data with percentages

    for state in states:
        # Filter the data by state
        state_data = data[data['STATE/UT'] == state]

        # Calculate total crimes for the state
        total_crimes_in_state = state_data['TOTAL IPC CRIMES'].sum()

        # Calculate crime percentage for each district
        state_data['Crime Percentage'] = (state_data['TOTAL IPC CRIMES'] / total_crimes_in_state) * 100

        # Append the result for this state
        results.append(state_data)

    # Concatenate all state dataframes
    return pd.concat(results, axis=0)

# Apply the function to calculate crime percentage for each district
crime_percentage_data = calculate_crime_percentage(data)

# Function to plot crime percentages as a pie chart for a given state
def plot_state_crime_percentage_pie(crime_percentage_data, state_name):
    # Filter data by state
    state_data = crime_percentage_data[crime_percentage_data['STATE/UT'] == state_name]

    # Plot pie chart
    plt.figure(figsize=(8, 8))
    plt.pie(state_data['Crime Percentage'], labels=state_data['DISTRICT'], autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired.colors)
    plt.title(f'Crime Percentage by District in {state_name}')
    plt.tight_layout()
    plt.show()

# Iterate over all states and plot pie charts for each state's districts
states = crime_percentage_data['STATE/UT'].unique()
for state in states:
    plot_state_crime_percentage_pie(crime_percentage_data, state)

# Save the result to a CSV file for reference
crime_percentage_data.to_csv('crime_percentage_data.csv', index=False)

pip install plotly

import pandas as pd
import plotly.express as px

# Load your dataset (replace 'crime_data.csv' with the actual file path)
data = pd.read_csv('/content/crime_india.csv')

# Select relevant columns (district, state, and total IPC crimes)
data = data[['STATE/UT', 'DISTRICT', 'TOTAL IPC CRIMES']]

# Function to calculate crime percentage for each district in a state
def calculate_crime_percentage(data):
    # Group data by state
    states = data['STATE/UT'].unique()  # Get unique states
    results = []  # To store each state's data with percentages

    for state in states:
        # Filter the data by state
        state_data = data[data['STATE/UT'] == state]

        # Calculate total crimes for the state
        total_crimes_in_state = state_data['TOTAL IPC CRIMES'].sum()

        # Calculate crime percentage for each district
        state_data['Crime Percentage'] = (state_data['TOTAL IPC CRIMES'] / total_crimes_in_state) * 100

        # Append the result for this state
        results.append(state_data)

    # Concatenate all state dataframes
    return pd.concat(results, axis=0)

# Apply the function to calculate crime percentage for each district
crime_percentage_data = calculate_crime_percentage(data)

# Function to plot an interactive pie chart for each state
def plot_interactive_pie_chart(state_name, crime_percentage_data):
    # Filter data for the selected state
    state_data = crime_percentage_data[crime_percentage_data['STATE/UT'] == state_name]

    # Create the pie chart using Plotly
    fig = px.pie(state_data,
                 names='DISTRICT',
                 values='Crime Percentage',
                 title=f'Crime Percentage by District in {state_name}',
                 hover_data=['TOTAL IPC CRIMES'],  # Add hover data for actual crime numbers
                 labels={'Crime Percentage': 'Percentage of Total Crimes'})

    # Customize the layout for better visualization
    fig.update_traces(textposition='inside', textinfo='percent+label', pull=[0.05]*len(state_data))

    # Show the figure
    fig.show()

# Example: Plot the interactive pie chart for a specific state (e.g., "Maharashtra")
plot_interactive_pie_chart("Maharashtra", crime_percentage_data)

# Iterate over all states and generate interactive charts for each state
states = crime_percentage_data['STATE/UT'].unique()
for state in states:
    plot_interactive_pie_chart(state, crime_percentage_data)

# Save the result to a CSV file for reference
crime_percentage_data.to_csv('crime_percentage_data.csv', index=False)

import pandas as pd
import plotly.express as px
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

# Load your dataset (replace 'crime_data.csv' with your file path)
data = pd.read_csv('/content/crime_india.csv')

# Select relevant columns (e.g., total IPC crimes or a combination of crime categories for clustering)
crime_columns = ['MURDER', 'RAPE', 'KIDNAPPING & ABDUCTION', 'ROBBERY', 'BURGLARY', 'THEFT', 'RIOTS', 'ASSAULT ON WOMEN WITH INTENT TO OUTRAGE HER MODESTY', 'TOTAL IPC CRIMES']

# Function to scale the crime data for clustering
def scale_crime_data(state_data):
    scaler = StandardScaler()
    return scaler.fit_transform(state_data[crime_columns])

# Function to apply K-Means clustering and return labels
def apply_kmeans(state_data, n_clusters=3):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    kmeans.fit(state_data)
    return kmeans.labels_

# Function to visualize clusters for a state
def visualize_clusters(state_name, state_data, cluster_labels):
    # Apply PCA to reduce to 2D for visualization (optional but recommended)
    pca = PCA(n_components=2)
    reduced_data = pca.fit_transform(state_data[crime_columns])

    # Create a DataFrame for visualization
    state_data['PCA1'] = reduced_data[:, 0]
    state_data['PCA2'] = reduced_data[:, 1]
    state_data['Cluster'] = cluster_labels

    # Plot the clusters using Plotly
    fig = px.scatter(state_data,
                     x='PCA1', y='PCA2',
                     color='Cluster',
                     hover_data=['DISTRICT', 'TOTAL IPC CRIMES'],
                     title=f'K-Means Clustering for Districts in {state_name}')

    fig.update_layout(title=f'Crime Clusters in {state_name} (K-Means)', xaxis_title='PCA1', yaxis_title='PCA2')
    fig.show()

# Function to apply clustering and visualization for each state
def cluster_and_visualize_all_states(data, n_clusters=3):
    states = data['STATE/UT'].unique()

    for state in states:
        # Filter the data by state
        state_data = data[data['STATE/UT'] == state].copy()

        # Scale the crime data
        scaled_data = scale_crime_data(state_data)

        # Apply K-Means clustering
        cluster_labels = apply_kmeans(scaled_data, n_clusters)

        # Visualize the clusters for the state
        visualize_clusters(state, state_data, cluster_labels)

# Apply clustering and visualize clusters for all states
cluster_and_visualize_all_states(data, n_clusters=3)

import pandas as pd
import plotly.express as px
data = pd.read_csv('/content/crime_india.csv')

def compare_total_crimes(data):
    # Group data by state and sum total IPC crimes
    state_crime_comparison = data.groupby('STATE/UT')['TOTAL IPC CRIMES'].sum().reset_index()

    # Create a bar chart for comparison
    fig = px.bar(state_crime_comparison,
                 x='STATE/UT',
                 y='TOTAL IPC CRIMES',
                 title='Comparison of Total IPC Crimes Across States',
                 labels={'TOTAL IPC CRIMES': 'Total IPC Crimes', 'STATE/UT': 'States'},
                 color='TOTAL IPC CRIMES',
                 text='TOTAL IPC CRIMES')

    fig.update_layout(xaxis_title='States', yaxis_title='Total IPC Crimes')
    fig.show()
compare_total_crimes(data)

import pandas as pd
import plotly.express as px

# Load your dataset
data = pd.read_csv('/content/crime_india.csv')

# List of crime categories to compare
crime_categories = ['MURDER', 'ATTEMPT TO MURDER', 'CULPABLE HOMICIDE NOT AMOUNTING TO MURDER',
    'RAPE', 'CUSTODIAL RAPE', 'OTHER RAPE', 'KIDNAPPING & ABDUCTION',
    'KIDNAPPING AND ABDUCTION OF WOMEN AND GIRLS', 'KIDNAPPING AND ABDUCTION OF OTHERS',
    'DACOITY', 'PREPARATION AND ASSEMBLY FOR DACOITY', 'ROBBERY',
    'BURGLARY', 'THEFT', 'AUTO THEFT', 'OTHER THEFT', 'RIOTS',
    'CRIMINAL BREACH OF TRUST', 'CHEATING', 'COUNTERFIETING',
    'ARSON', 'HURT/GREVIOUS HURT', 'DOWRY DEATHS', 'ASSAULT ON WOMEN WITH INTENT TO OUTRAGE HER MODESTY',
    'INSULT TO MODESTY OF WOMEN', 'CRUELTY BY HUSBAND OR HIS RELATIVES',
    'IMPORTATION OF GIRLS FROM FOREIGN COUNTRIES', 'CAUSING DEATH BY NEGLIGENCE',
    'OTHER IPC CRIMES', 'TOTAL IPC CRIMES']

def compare_individual_crimes(data, crime_categories):
    for crime in crime_categories:
        # Group data by state and sum the individual crime category
        state_crime_comparison = data.groupby('STATE/UT')[crime].sum().reset_index()

        # Create a bar chart for comparison
        fig = px.bar(state_crime_comparison,
                     x='STATE/UT',
                     y=crime,
                     title=f'Comparison of {crime} Across States',
                     labels={crime: crime, 'STATE/UT': 'States'},
                     color=crime,
                     text=crime)

        fig.update_layout(xaxis_title='States', yaxis_title=crime)
        fig.show()

# Run comparison for individual crime categories
compare_individual_crimes(data, crime_categories)

import pandas as pd
import plotly.express as px

# Load your dataset
data = pd.read_csv('/content/crime_india.csv')

# List of crime categories to compare
crime_categories = [
    'MURDER', 'ATTEMPT TO MURDER', 'CULPABLE HOMICIDE NOT AMOUNTING TO MURDER',
    'RAPE', 'CUSTODIAL RAPE', 'OTHER RAPE', 'KIDNAPPING & ABDUCTION',
    'KIDNAPPING AND ABDUCTION OF WOMEN AND GIRLS', 'KIDNAPPING AND ABDUCTION OF OTHERS',
    'DACOITY', 'PREPARATION AND ASSEMBLY FOR DACOITY', 'ROBBERY',
    'BURGLARY', 'THEFT', 'AUTO THEFT', 'OTHER THEFT', 'RIOTS',
    'CRIMINAL BREACH OF TRUST', 'CHEATING', 'COUNTERFIETING',
    'ARSON', 'HURT/GREVIOUS HURT', 'DOWRY DEATHS', 'ASSAULT ON WOMEN WITH INTENT TO OUTRAGE HER MODESTY',
    'INSULT TO MODESTY OF WOMEN', 'CRUELTY BY HUSBAND OR HIS RELATIVES',
    'IMPORTATION OF GIRLS FROM FOREIGN COUNTRIES', 'CAUSING DEATH BY NEGLIGENCE',
    'OTHER IPC CRIMES', 'TOTAL IPC CRIMES'
]

def compare_yearly_crimes(data, crime_categories):
    for crime in crime_categories:
        # Group data by state and year, then sum the individual crime category
        state_crime_comparison = data.groupby(['STATE/UT', 'YEAR'])[crime].sum().reset_index()

        # Create a bar chart for comparison
        fig = px.bar(state_crime_comparison,
                     x='YEAR',
                     y=crime,
                     color='STATE/UT',  # Use STATE/UT for color
                     title=f'Year-wise Comparison of {crime} Across States',
                     labels={crime: crime, 'Year': 'YEAR'},
                     text=crime)

        fig.update_layout(xaxis_title='Year', yaxis_title=crime)
        fig.show()

# Run comparison for individual crime categories with year-wise data
compare_yearly_crimes(data, crime_categories)

import pandas as pd
import plotly.express as px

# Load your dataset
data = pd.read_csv('/content/crime_india.csv')

# Ensure your dataset has the following columns: ['STATE/UT', 'DISTRICT', 'YEAR', <crime categories>]
# For example, it should look like this:
# | STATE/UT | DISTRICT | YEAR | MURDER | RAPE | ... | TOTAL IPC CRIMES |

# List of crime categories to compare
crime_categories = [
    'MURDER', 'ATTEMPT TO MURDER', 'CULPABLE HOMICIDE NOT AMOUNTING TO MURDER',
    'RAPE', 'CUSTODIAL RAPE', 'OTHER RAPE', 'KIDNAPPING & ABDUCTION',
    'KIDNAPPING AND ABDUCTION OF WOMEN AND GIRLS', 'KIDNAPPING AND ABDUCTION OF OTHERS',
    'DACOITY', 'PREPARATION AND ASSEMBLY FOR DACOITY', 'ROBBERY',
    'BURGLARY', 'THEFT', 'AUTO THEFT', 'OTHER THEFT', 'RIOTS',
    'CRIMINAL BREACH OF TRUST', 'CHEATING', 'COUNTERFIETING',
    'ARSON', 'HURT/GREVIOUS HURT', 'DOWRY DEATHS', 'ASSAULT ON WOMEN WITH INTENT TO OUTRAGE HER MODESTY',
    'INSULT TO MODESTY OF WOMEN', 'CRUELTY BY HUSBAND OR HIS RELATIVES',
    'IMPORTATION OF GIRLS FROM FOREIGN COUNTRIES', 'CAUSING DEATH BY NEGLIGENCE',
    'OTHER IPC CRIMES', 'TOTAL IPC CRIMES'
]

def compare_yearly_crimes_by_district(data, crime_categories):
    # Get unique years from the dataset
    years = data['YEAR'].unique()

    for year in years:
        # Filter data for the specified year
        year_data = data[data['YEAR'] == year]

        for crime in crime_categories:
            # Group data by state, district and sum the individual crime category
            district_crime_comparison = year_data.groupby(['STATE/UT', 'DISTRICT'])[crime].sum().reset_index()

            # Create a bar chart for comparison
            fig = px.bar(district_crime_comparison,
                         x='DISTRICT',
                         y=crime,
                         color='STATE/UT',  # Use STATE/UT for color
                         title=f'{year} Year-wise Comparison of {crime} by Districts',
                         labels={crime: crime, 'DISTRICT': 'Districts'},
                         text=crime)

            fig.update_layout(xaxis_title='Districts', yaxis_title=crime)
            fig.show()

# Run comparison for individual crime categories for all years
compare_yearly_crimes_by_district(data, crime_categories)



import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import plotly.express as px

# Load your dataset (uncomment and update the file path if needed)
data = pd.read_csv('/content/crime_india.csv')

# Ensure relevant columns are present
required_columns = ['STATE/UT', 'DISTRICT', 'YEAR', 'MURDER', 'RAPE']
if not all(col in data.columns for col in required_columns):
    raise ValueError(f"Data must contain the following columns: {required_columns}")

# Aggregate data by STATE/UT, DISTRICT, and YEAR
crime_aggregated = data.groupby(['STATE/UT', 'DISTRICT', 'YEAR']).sum().reset_index()

# Select relevant features for clustering (you can modify this based on your dataset)
features = ['MURDER', 'RAPE']  # Add more features if needed

# Get unique states for iteration
states = crime_aggregated['STATE/UT'].unique()

# Store summaries for each state
summaries = []

for state in states:
    # Filter data for the specific state
    state_data = crime_aggregated[crime_aggregated['STATE/UT'] == state]

    # Check if there are enough data points to cluster
    if len(state_data) < 2:
        print(f"Not enough data for clustering in {state}.")
        continue

    # Standardize the features
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(state_data[features])

    # Applying K-Means (you can adjust n_clusters based on your analysis)
    n_clusters = 5  # Adjust as needed
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    state_data['Cluster'] = kmeans.fit_predict(X_scaled)

    # Create a summary of cluster results
    cluster_summary = state_data.groupby('Cluster').agg({
        'MURDER': 'mean',
        'RAPE': 'mean',
        'DISTRICT': 'count'  # Count of districts in each cluster
    }).reset_index()

    cluster_summary.rename(columns={'DISTRICT': 'District Count'}, inplace=True)

    # Identify the most populous cluster
    top_cluster = cluster_summary.loc[cluster_summary['District Count'].idxmax()]
    summary_statement = (
        f"In {state}, the most populous cluster has {top_cluster['District Count']} districts, "
        f"averaging {top_cluster['MURDER']:.2f} murder cases and {top_cluster['RAPE']:.2f} rape cases."
    )

    # Store the summary statement
    summaries.append(summary_statement)

    # Visualizing clusters with Plotly
    fig = px.scatter(state_data, x='MURDER', y='RAPE', color='Cluster',
                     title=f'K-Means Clustering of Crime Data for {state}',
                     labels={'MURDER': 'Murder Cases', 'RAPE': 'Rape Cases'},
                     hover_data=['DISTRICT'])
    fig.show()

# Print the summaries for all states
for summary in summaries:
    print(summary)

